"""
Given a vocab file, generate the rest of unused tokens to make it a total of 30522 lines
"""

import sys

meta_tokens = [
    "[S]",
    "[E]",
    "[TIMESTAMP]",
    "[OPCODE]",
    "[RS1]",
    "[RS2]",
    "[RD]",
    "[FUNCT3]",
    "[FUNCT7]",
    "[IMM]",
]

riscv_vocabs = [
    "unimp",
    "ebreak",
    "sbreak",
    "ret",
    "jr",
    "jalr",
    "j",
    "jal",
    "call",
    "tail",
    "jump",
    "nop",
    "lui",
    "li",
    "mv",
    "move",
    "zext",
    "andi",
    "and",
    "beqz",
    "beq",
    "blez",
    "bgez",
    "bge",
    "bgeu",
    "ble",
    "bleu",
    "bltz",
    "bgtz",
    "blt",
    "bltu",
    "bgt",
    "bgtu",
    "bnez",
    "bne",
    "addi",
    "add",
    "la",
    "lla",
    "lga",
    "la",
    "neg",
    "slli",
    "sll",
    "srli",
    "srl",
    "srai",
    "sra",
    "sub",
    "lb",
    "lbu",
    "lh",
    "lhu",
    "lw",
    "not",
    "ori",
    "or",
    "lpad",
    "auipc",
    "seqz",
    "snez",
    "sltz",
    "sgtz",
    "slti",
    "slt",
    "sltiu",
    "sltu",
    "sgt",
    "sgtu",
    "sb",
    "sh",
    "sw",
    "fence",
    "rdcycle",
    "rdinstret",
    "rdtime",
    "rdcycleh",
    "rdinstreth",
    "rdtimeh",
    "ecall",
    "scall",
    "xori",
    "xor",
    "lwu",
    "ld",
    "sd",
    "sext",
    "addiw",
    "addw",
    "negw",
    "slliw",
    "sllw",
    "srliw",
    "srlw",
    "sraiw",
    "sraw",
    "subw",
    "mul",
    "mulh",
    "mulhu",
    "mulhsu",
    "div",
    "divu",
    "rem",
    "remu",
    "mulw",
    "divw",
    "divuw",
    "remw",
    "remuw",
    "c.unimp",
    "c.ebreak",
    "c.jr",
    "c.jalr",
    "c.j",
    "c.jal",
    "c.beqz",
    "c.bnez",
    "c.lwsp",
    "c.lw",
    "c.swsp",
    "c.sw",
    "c.nop",
    "c.nop",
    "c.mv",
    "c.lui",
    "c.li",
    "c.addi4spn",
    "c.addi16sp",
    "c.addi",
    "c.add",
    "c.sub",
    "c.and",
    "c.or",
    "c.xor",
    "c.slli",
    "c.srli",
    "c.srai",
    "c.slli64",
    "c.srli64",
    "c.srai64",
    "c.andi",
    "c.addiw",
    "c.addw",
    "c.subw",
    "c.ldsp",
    "c.ld",
    "c.sdsp",
    "c.sd",
    "c.fldsp",
    "c.fld",
    "c.fsdsp",
    "c.fsd",
    "c.flwsp",
    "c.flw",
    "c.fswsp",
    "c.fsw",
    "x0",
    "x1",
    "x2",
    "x3",
    "x4",
    "x5",
    "x6",
    "x7",
    "x8",
    "x9",
    "x10",
    "x11",
    "x12",
    "x13",
    "x14",
    "x15",
    "x16",
    "x17",
    "x18",
    "x19",
    "x20",
    "x21",
    "x22",
    "x23",
    "x24",
    "x25",
    "x26",
    "x27",
    "x28",
    "x29",
    "x30",
    "x31",
    "ra",
    "sp",
    "gp",
    "tp",
    "t0",
    "t1",
    "t2",
    "s0",
    "fp",
    "s1",
    "a0",
    "a1",
    "a2",
    "a3",
    "a4",
    "a5",
    "a6",
    "a7",
    "s2",
    "s3",
    "s4",
    "s5",
    "s6",
    "s7",
    "s8",
    "s9",
    "s10",
    "s11",
    "t3",
    "t4",
    "t5",
    "t6",
    "timestamp:",
    "instr:",
]


out_file = "vocab.txt"


vocab_file_content = ""

# meta tokens
for token in meta_tokens:
    vocab_file_content += f"{token}\n"

# numbers
vocab_file_content += f"-\n"
for i in range(1000):
    vocab_file_content += f"{i}\n"

# riscv vocabs
for vocab in riscv_vocabs:
    vocab_file_content += f"{vocab}\n"


num_lines = len(vocab_file_content.split("\n"))

# Calculate how many unused tokens to add
unused_lines = 30523 - num_lines
print(f"Current vocabulary has {num_lines} tokens. Adding {unused_lines} unused tokens.")

# Then append the unused tokens
with open(out_file, "w") as f:
    f.write(vocab_file_content)
    for i in range(unused_lines):
        f.write(f"[unused{i}]\n")

print(f"Updated vocabulary file now has 30522 tokens.")
